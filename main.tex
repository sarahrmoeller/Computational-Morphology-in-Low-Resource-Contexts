\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T2A, T1]{fontenc}
\usepackage{expex}
\lingset{aboveglftskip=-.2ex,interpartskip=\baselineskip,everyglb=\normalsize}
\usepackage[russian,english]{babel}
\usepackage{tipa}
\usepackage{coling2018}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{longtable}
\usepackage{textcomp}
\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{setspace}
\doublespacing
\pagenumbering{arabic}
\pagestyle{plain}

\title{Computational Morphology in Low Resource Settings}
\author{\\Synthesis Paper \\ Sarah R. Moeller}
\date{}

\begin{document}

\maketitle

\section{Introduction}

 This paper examines how computational methods can be applied to the analysis of morphology in low resource contexts, particularly with language documentation and description (LDD) data.\footnote{``Language documentation and description'' (LDD) data refers to resources that results from linguistic fieldwork and basic linguistic analysis. ``low resource languages'' (LRL), according to LORELEI (a US-government funded project for building language technology in low resource languages), refers to languages for which no automated human language technology exists, typically because of a lack of available linguistic resources. Other terms are sometimes used: ``under-described languages'' could be described as having minimal published descriptive linguistic resources, what \newcite{duong_natural_2017} calls ``very scarce-resource language;'' ``under-documented languages'', or Duong's ``extremely scarce-resource languages,'' lack sufficient raw or annotated data to write a full descriptive grammar; ``endangered languages'' are predicted to have no native speakers within a generation or two and most are under-documented and/or under-described. These distinctions are rarely crucial in this paper and can be understood almost interchangeably.} It attempts to identify ``[w]hat [computational] methods…can [and have been used to] detect [morphological] structure in small, noisy data sets, while being directly applicable to a wide variety of languages'' \cite[page 472]{bird_natural_2009}. Such methods have the potential speed and improve language documentation and language description and to determine where natural language processing (NLP) methods could grow beyond the current state of the art. 

Morphology comprises word-building properties in human languages and their accompanying (morpho-)syntactic phenomena. Historically, computational linguists and ``paper-and-pencil linguists'' have taken different and sometimes seemingly incompatible approaches to morphology \cite[page 80]{karttunen_2005}. In general linguistics, analyzing the morphological system is a first step in describing a language. Morphological description of a broad range of languages must support any reasonable linguistic theory. Computational morphology attempts to parse or generate valid inflected forms according to the language's morphological structure in order to improve downstream NLP tasks such as machine translation or voice recognition. Among some computational linguists there seems to be a belief that linguistic theory has little benefit for computational methods \cite{goldsmith_computational_2017}. Hypothesizing an all-encompassing theory of language does not induce from data a workable NLP tool. It has even been questioned whether computational linguistics needs morphological analysis at all, particularly with the recent trend of end-to-end training which have the potential to improve downstream tasks without explicitly modeling morphology. Might NLP goals benefit equally well by representing words as strings of characters? Although character-level models can learn relationships between orthographically similar words, comparing results on ten language shows that such models are too easily led up the garden path by orthographic signals \cite{vania_characters_2017}. Language models trained on morphological patterns provide greater predictive accuracy.  

Despite their out-of-sync approaches, both computational linguistics and ``traditional'' linguistics benefit from morphological analysis \cite[page 165]{cotterell_labeled_2015}. What's more, the field of linguistics is not interested solely in establishing a theory of language. Early linguistics focused on increasing human understanding of language by describing a language's structure, including morphology, using recorded data. This focus has been revived since the 1990's with language documentation, a subfield that is also keenly interested in practical downstream goals such as language technology, language conservation, and language learning.\footnote{A casual perusal of the flagship journal \textit{Language Documentation and Conservation} journal attests this interest.}

Morphological analysis is particularly important for morphologically complex languages, such as agglutinating (common in central and north Asia, South America, central and southern Africa, Australia), polysynthetic (North America, the Far East of Russia), or non-concatenative (north Africa and the Middle East, southeast Asia). Languages that build words from multiple morphemes or via significant morphophonological changes produce a high number of inflected and compound words which appear to the machine as brand new, unrelated words \cite{dreyer_discovering_2011,goldsmith_computational_2017,hammarstrom_unsupervised_2011,kann_neural_2016,ruokolainen_supervised_2013}. The fact that computational linguists feel able to ask whether morphological analysis is even necessary hints at the out-sized role that so-called high resource languages have played in natural language processing. The most common languages used in computational linguistics are European languages, Chinese, and Arabic, most of which have comparatively simple fusional or isolating morphology, with a very few (e.g. Finnish) that belong to the somewhat more complicated agglutinative or non-concatenative types (i.e. Arabic). None belong to the much more complicated polysynthetic type, none are under-documented, none are endangered languages. While field of linguistics has slowly but surely widened the world's knowledge about human language, thanks in part to a recent emphasis on documentary fieldwork, computational linguistics has yet to truly expand beyond a handful of economically or politically powerful languages. As an example, when \newcite{vania_characters_2017} compared morpheme-level versus character-level representation, they selected 10 languages that in fact represent quite a wide range of morphological phenomena, but still only from six language families. Three are Indo-European languages characterized by fusional morphology (though English tends toward the even simpler isolating type). One (Finnish) is a European, but not Indo-European, language with agglutinative morphology and many other interesting morphological and phonological alternations. Four others (Japanese, Turkish, Indonesian and Malaysian) are also agglutinative. The other two languages (Hebrew and Arabic) are Semitic languages, famous for the templatic type of non-concatenative morphology. Even this narrow dataset demonstrated that the more complex a language’s morphology is, the more crucial the role that morphological analysis plays in the final results. 
 
The rest of this paper assumes that morphology does have a role to play in both computational and general linguistics. Section \ref{tasks} defines and compares the tasks of morphology learning in computational linguistics and language documentation and description. This paper also assumes that NLP algorithms and methods should be expanded into more low resource languages. Section \ref{compMorph} reviews the literature on various computational approaches to morphology, how they have been applied to low resource languages, and what their advantages and disadvantages are in low resource settings. Two issues arise throughout the paper. First, how can machine learning be improved when data is limited? Second, what are the most promising computational methods for LDD? In addition to the reviews in section \ref{compMorph}, section \ref{resources} presenting specific techniques that have overcome data limitations. Section \ref{CLLDD} addresses the second question but goes beyond specific methods by envisioning how computational linguistics and language documentation and description could together address the issue of limited data which plagues both fields.

\section{Morphology}
\label{tasks}

Historically, the study of morphology is the inference of rules that govern a language’s word building strategies including morphophonological changes in morpheme shapes (pronunciation), and the discovery of systematically related word forms (derivational morphology and inflectional paradigms) \cite{roark_computational_2007}. According to \newcite{goldsmith_computational_2017}, an ideal morphological model answers the following questions:\begin{singlespace}

\smallskip
\begin{itemize}
    \item What are the morphemes in each word? What are their meanings or functions? 
    \item What morphological paradigms exist in the language? What morphological features distinguish them from one another?
    \item What allomorphs, or alternative pronunciations, does each morpheme have? Under what conditions do the allomorphs appear?  
    \item What combinations of morphemes does the language permit on each lexical category (part of speech (POS))?
    \item What are the morphological processes that significantly change a word's meaning or part of speech? How productive are these derivational processes?
\end{itemize}
\end{singlespace}
\smallskip

Such a complete morphological model is difficult to accomplish and to evaluate computationally. It would need to be computable, robust, interpretable, while accounting for all the language's morphophonogy, allomorphy, and for any ambiguous inflected forms \cite{virpioja_empirical_2011}. In other words, a computational model that reaches the ideal would be nearly as good as human descriptive linguists, who are also able to account for the ever-present inconsistencies and unsystematic exceptions that occur in natural languages. While descriptive linguists attempt to answer all the above questions (and more), most effort in computational morphology has been expended on the first set of questions, with some significant effort given to the second. This section compares how computational linguistics and language documentation and description (LDD) approach these two sets of questions and their related tasks.

Attempts to answer the first set of questions about the number, shape, and meaning of morphemes in a language is the core part of morphological analysis. Traditionally, morphemes are first identified, for example, \textit{-ed} would be recognized as a word segment with its own minimal meaning. Whether or not individual morphemes are identified, the meanings, or functions, that each morpheme contributes to a word must be deduced. For example, \textit{-ed} can mean `\textsc{past}' or `\textsc{participle}'. After this, it becomes possible to examine questions about morphological paradigms, delving deeper into a language's morphology. Systematic rules that govern how morphemes combine on a word and which can or cannot co-occur are inferred from the analyzed data. For example, once the morpheme is identified in natural occurring texts, it should be clear that English uses \textit{-ed} as a suffix to add past tense or participial meaning to a verb stem. Subsets of rules may govern certain classes of morphemes and these classes and their morphological patterns can be exemplified by paradigms such as Table \ref{tab:RuParadigm}. Thus, for example, the class of ``regular'' English uses the suffix \textit{-ed} (versus ``irregular'' verbs: \textit{swim, swam, swum}, etc.). 

\subsection{Morphological Analysis and Annotation}
\label{analysis}

Morphological analysis can be separated into two core tasks \cite{cotterell_labeled_2015,hammarstrom_unsupervised_2011,nicolai_morphological_2017,palmer_semi-automated_2009}. The first task is identifying morphemes by determining their shapes and marking boundaries between them, as done for the Lezgi [lez] noun in (\ref{ex:Lezgi1b}). The task is sometimes called (unlabeled) morpheme segmentation \cite{creutz_unsupervised_2007,snyder_unsupervised_2008}. The second task is deducing each morpheme's meaning, known as parsing, or sometimes, by itself, as morphological analysis. If morpheme segmentation is done first, the second task usually involves associating each morpheme with a label (gloss or tag) that indicates its meaning or function as, for example, the \textsc{obl} (oblique stem) and \textsc{gen} (genitive case) in (\ref{ex:Lezgi1c}). This is sometimes known as glossing, tagging, or labeled morpheme segmentation. These two tasks are a large part of linguistic data annotation. 

\begin{singlespace}
%\pex<withparts> %% "main" example needs a tag
%\label{ex:Lezgi1}
%\a<first> %% First part with a tag
%\begingl 
%\gla pa\c{c}ahdin //
%\glb pa\c{c}ah-di-n //
%\glc king-\textsc{obl}-\textsc{gen} //
%\glft `king's'//
%\endgl

%\a<second> \ljudge{*} %% Second part
%\begingl %% start glosses
%\gla \textbf{syá} luto/lwito//
%\glb \textsc{di} 11ashes//
%\endgl
%\xe

\pex<Lezgi1>   
\label{ex:Lezgi1}
\a<a> pa\c{c}ahdin 
\label{ex:Lezgi1a}
\a<b> pa\c{c}ah-di-n 
\label{ex:Lezgi1b}
\a<c> king-\textsc{obl}-\textsc{gen} 
\label{ex:Lezgi1c}
\a `king's'
\label{ex:Lezgi1d}
\xe

\end{singlespace}

Discovering morphemes'  meanings does not necessarily require the first task: morpheme identification and segmentation. It is possible to parse a word's meaning without identifying the individual morphemes which compose that meaning.\footnote{One field linguist (p.c.) claims the reverse is possible - native speakers could segment words into their minimally meaningful parts without being able to identify those meanings.  But it does not seem likely that this would achieve good results since the speaker could not grasp the semantic or syntactic factors that determine a morpheme’s shape.} Performing the second task without the first task is much more common in computational approaches. Computational morphology aims to take texts from any human language and build a model that accounts for every word in them and generalizes with high accuracy to unseen words in new texts \cite{goldsmith_computational_2017}. If the immediate task can be accomplished without it, a model may skip morpheme identification and segmentation, as, for example, in one CoNLL-SIGMORPHON 2018 shared task, shown in (\ref{ex:ConLL-T2}), where words were inflected without identifying the language's morphemes \cite{cotterell_conllsigmorphon_2018}.

\begin{singlespace}
\pex<ConLL-T2>   
\label{ex:ConLL-T2}
\a<a> \textbf{INPUT:}  The \rule{1cm}{0.15mm} are barking.  \hspace{5 mm} \textit{dog}
\label{ex:ConLL-T2a}
\a<b> \textbf{OUTPUT:} dogs
\label{ex:ConLL-T2b}
\xe
\end{singlespace}

\newcite{nicolai_morphological_2017} divide the morphological analysis subtasks slightly differently, making a distinction between morphological “analysis” and morphological tagging. They describe morphological analysis as a combination of segmentation and labeling, but later state that ``morphological tagging can be performed as a downstream application of morphological analysis'' \newcite[page 211]{nicolai_morphological_2017}, thereby adhering the same two distinctions described above. \newcite{virpioja_empirical_2011} adds a third task under morphological analysis: identification of morphologically related words. Identifying morphologically related words is essentially a first step in identifying inflectional classes and the systematic rules that govern them (see section \ref{paradigms}. 

In computational morphology, the two tasks, if both are attempted, are often tackled sequentially. First, the machine finds morpheme boundaries, then attaches labels. In a narrow sense, since a machine cannot make analytical decisions that a human can (though it can find complicated patterns and make predictions which may seem like analytical decisions), all computational morphology could be seen as annotation. However, some computational models have taken a tip from LDD workflow and united the two tasks. In some cases, this yields higher accuracy. It also allows the machine to go beyond annotation and ``learn a probabilistic model of morphotactics [rules about the ordering and cooccurrence of morphemes on a word]'' \cite[page 165]{cotterell_labeled_2015}.

Computationally, morpheme segmentation algorithms are quite similar to word segmentation. Segmentation provides a lexicon of morphemes which is more generalizable than a vocabulary of word forms as they appear in running text \cite{creutz_unsupervised_2002}. Segmentation divides strings of text without regard to their potential relation \cite{virpioja_empirical_2011}. Computational approaches can be lexicon-based, focusing on detecting morpheme shapes or the approach can be a model that focuses on detecting morpheme boundaries \cite{goodman_generation_2013}. Most lexicon-based approaches learn a generative model. They create a model of word forms and can generate them. Most boundary detection models perform discriminative learning, probabilistically estimating the segmentation boundaries in a given word. Computational morphological segmentation tends to perform better on concatenative morphology, where words are built from morphemes like beads on a string such as in example \ref{ex:Lezgi1}. Computational modeling of non-concatenative morphology, particularly when it occurs in a language that combines with concatenative morphology, remains ``arguably one of the main current challenges of the field'' and attempts ``have been mostly applied to Arabic'' \cite{goldsmith_computational_2017}. 

 During morpheme segmentation, the machine is not usually tasked to identify allomorphy, though some learning of allomorphy and morphophonogical changes may happen before this step \cite{goldsmith_computational_2017}. Ideally, the second task of morphological parsing or labeling accounts for allomorphy and other complicated morphophonogical issues. This is difficult to do without some previous analysis so it is no surprise that the most accurate, and historically most popular, computational morphological models are finite state transducers (FSTs) because they can take advantage of published linguistic descriptions and manually construct a complete morpheme lexicon and collection of a language's morphophonogical rules. 

In descriptive linguistics, the two tasks are not usually distinguished because they are typically tackled simultaneously. It is more likely that general linguistics would categorize as morphological analysis all the tasks leading to the identification of morphemes, their meanings, and the description of systematic rules and relationships between words. The mechanical parts of these tasks---segmenting and glossing morphemes---are sometimes distinguished together as (morphological) annotation. In language documentation and description, annotation produces interlinear glossed texts (IGT). IGTs stand on the fuzzy line between documentary linguistics and descriptive linguistics. They go beyond mere documentation of a language, yet are still a ``preprocessing step'' to describing a language's structure \cite{moon_unsupervised_2009}. By convention, IGTs consist of three lines: a line of text with morpheme boundaries marked by hyphens, a line of the morpheme glosses aligned below each morphemes, and a line of a rough, free translation of the text into a language of wider communication. 

Morphological segmentation and tagging should be distinguished from syntactic annotation, of which the most common type is part-of-speech (POS) tagging, but this distinction can be problematic because some POS tag sets such as the expanded set of Universal Dependency tags \cite{de_marneffe_universal_2014} include morphologically-marked syntactic features, such as number or grammatical case. In fact, ``morphological'' tag sets and ``morphological'' annotation could benefit from even more syntactic information \cite{cotterell_cross-lingual_2017}. LDD distinguishes morphological annotation from syntactic annotation even less strictly than computational linguistics. IGTs often include lexical categories (POS) and information related to syntax, phonology, and even non-verbal communication. The fact is that the divisions (morphology, syntax, phonology, etc.) are convenient for linguistic science but are not as clear in natural language. The division between morphology and syntax is more obvious in the relatively simple morphological structures that many high resource languages have, such as the fusional morphology of most European languages or the isolating morphology of Chinese languages. Both fusional and isolating morphologies use independent function words (e.g. prepositions, particles, etc.) rather than bound morphemes to express a significant amount of syntactic information. Even some more complex morphological structures, such as the non-concatenative, templatic Arabic, also expresses a fair amount of syntactic information with independent words. These strategies stand in contrast to agglutinative and polysynthetic morphology characterizing many low resource languages, which express most or all syntactic information by bound morphemes on the inflected word forms. 

\subsection{Morphological Paradigm Induction}
\label{paradigms}

The study of a language's morphology begins with the analysis of morphemes but extends to the inference of the system of rules that dictate how morphemes build words and interact with each other. This inference is based on three assumptions \cite{durrett_supervised_2013}. First, within a lexical category groups of lexemes inflect according to a pattern that is dictated by subsystem of rules. Russian nouns, for example, can be generalized into three simplified patterns of inflection, shown in Table \ref{tab:RuParadigm}. Patterns in a table like this is sometimes called inflectional paradigms. Second, these patterns are triggered by context and the morphological rules governing the patterns can be inferred from the triggering context. Descriptive studies look to phonology for the triggering context or some combination of phonological structure and the semantic content of the lexemes. Computational models, due to the nature of their input texts, hinge upon orthographic context. Third, a stem morpheme is consistently inflected. Understanding a language's morphology means describing inflectional patterns and grouping lexemes that inflect according to a subsystem of rules. These groups of morphemes are inflectional classes (sometimes called ``declensions'' for nouns and adjectives and ``conjugations'' for verbs). A class's complete inflectional pattern is described by an inflectional paradigm which display either just the inflectional affix(es), as in Table \ref{tab:RuParadigm}, or a lexeme with all its inflected form. 

\begin{table}[ht]
\begin{center}
%\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{c|cc|cc|cc}
\toprule
{} & \multicolumn{2}{c|}{\bf Class 1} & \multicolumn{2}{c|}{\bf Class 2} & \multicolumn{2}{c}{\bf Class 3} \\
{}	& \textsc{sg} & \textsc{pl}	& \textsc{sg} & \textsc{pl} & \textsc{sg} & \textsc{pl} \\
\midrule
\textsc{nom} & -a & -\textbari & \O & -\textbari & -\textsuperscript{j} & -i \\
\textsc{acc} & -u & -\textbari /\O  & \O/-a & -\textbari/-ov & -\textsuperscript{j} & -i/-jej \\
\textsc{gen} & -\textbari & \O & -a & -ov & -i & -jej \\
\textsc{dat} & -je & -am & -u & -am & -i & -jam \\
\textsc{inst} & -oj & -ami & -om & -ami & -ju & -jami \\
\textsc{prep} & -je & -ax & -je & -ax & -i & -jax \\
\bottomrule
\end{tabular}
\end{center}
\caption{Example of Inflectional Paradigms for Russian Nouns. The second row and leftmost column indicate the morphosyntactic features that each slot rerpesents.}
\label{tab:RuParadigm}
\end{table}

These tables are challenging to induce from raw data. Within one language, inflectional classes are not always unique or regular, they may have overlapping patterns and isomorphic morphemes that result in ambiguous forms. For example, an Arapaho verb ending in \textit{-\'ot} might mean ``You alone do something to \underline{him/her/it}'' or it might mean ``You alone do something to \underline{them}.'' Some languages use suppletive forms that have no similarity in shape to their ``base'' form (e.g. English ``go'' vs ``went''). Field linguists can refer these difficulties to native speakers but machine inputs are typically limited to written texts sourced from published material.

\newcite{monson_paramorfinding_2007b} state two guiding principles for computational paradigm induction that are applicable for any approach that is limited to textual input. First, ``in any given corpus, a particular lexeme will likely not occur in all possible inflected forms''.  Even with a large corpus, attempts to recreate a paradigm as in Table \ref{tab:RuParadigm} will result in empty gaps. Language documentation best practice field methods encourages elicitation of inflectional paradigms in focused sessions---despite the subfield's emphasis on natural language---precisely because complete paradigms so rarely appear in natural language \cite{lupke_data_2010}. Certain inflectional forms will be much more common than others. In fact, for some lexemes, certain inflectional forms may never occur in spoken language even though they are grammatically possible \cite{silfverberg_encoder-decoder_2018}. It may be possible to find all inflectional forms of the most frequent lexemes, but frequent words may follow irregular patterns such as the English ``be'' verb. Therefore, paradigms need to be completed by inducing the patterns from several incomplete paradigms. The second principle is that we expect inflected forms of a single lexeme to be similar. This second principle of consistency plays an important role when identifying related words forms. It is not always true---languages abound with exceptions---but it is a solid working assumption.  

Descriptive linguistics infer inflectional paradigms from annotated data. An ideal analysis strives to describe all possible patterns of inflection. Theoretically, this means that every inflectional form of every word needs to be examined, but in practice, paradigmatic patterns can be inferred fairly quickly and the linguist can then concentrate on matching lexemes to a paradigm and identifying irregularities. Most analysis is done with spreadsheets and charts in computer programs such as Microsoft Excel. 

Computational models have successfully induced frequent and regular paradigms with high accuracy even in low resource settings \cite{hammarstrom_unsupervised_2011,durrett_supervised_2013,ahlberg_semi-supervised_2014}. Most early work on computational morphological paradigm induction applied unsupervised learning to concatenative morphology \cite{goldsmith_unsupervised_2001,chan_learning_2006,monson_paramorfinding_2007b}. Supervised and semi-supervising models have been more recently applied on concatenative and non-concatenative languages \cite{dreyer_discovering_2011,durrett_supervised_2013}. ParaMor \cite{monson_paramorMinimally_2007a} is an example of an unsupervised model. It begins by searching the data for sets of strings that resemble inflectional paradigms. It does this by identifying candidate ``suffixes'' in the word-final substrings. A candidate suffix is a substring that repeats at the end of different strings. The sets are refined to become partial paradigms. Inflectional paradigms induced by unsupervised learning like this has three flaws. First, suffixes may be in multiple paradigms because suffixes overlap between partial paradigms, for example ``-s'' on an English word may mark plurality on a noun or third person singular subject agreement and present tense on a verb. Second, most predicted partial paradigms contain ``many fewer candidate suffixes than do the true paradigms'' \cite[page 903]{monson_paramorfinding_2007b}. This is another way of stating Monson's first principle: induction from a corpus of natural text leaves gaps in paradigms because some inflected forms are rare or never used and some are very frequent. Third, some suffixes are inevitably identified at incorrect morpheme boundaries. For example, the English word ``ally'' could be easily misidentified as a root ``all'' plus the adverbializing suffix  ``-y''. \newcite{monson_paramorfinding_2007b} solve the first two flaws by leveraging Monson's second principle of relatedness and similar inflection and clustering candidate suffixes with a similarity score. The third flaw is addressed by filtering paradigms that do not include a threshold number of forms.  

Paradigm learning with supervised machine learning means at least partially complete paradigm tables are needed to train a model. A common source of inflectional tables is Wiktionary, a crowd-sourced online dictionary. Wiktionary contributors usually include an inflectional table for each entry. Sometimes, if known, the lexeme is identified with its paradigm or inflectional class. Some supervised learning has been applied to tasks related to paradigm learning but that are not direct attempts to identify a language's inflectional classes. They focus on generating correct inflected forms. For example, \newcite{malouf_generating_2016} applied supervised deep learning in the form of recurrent neural networks to answer the Paradigm Cell Filling Problem (PCFP) \cite{Ackerman_partsand}, illustrated in Figure \ref{fig:PCFP}, which asks how new speakers infer the inflected forms of a lexeme when they have not seen all forms? Malouf et al.'s model took as input the ``base'' form represented as a one-hot vector and the morphosyntactic features tags of the inflected form to generate. For example, if the input is a representation of ``walk'' and the features tense = \textsc{pres}, person = \textsc{3}, number = \textsc{sg}, then the correct output is ``walks''. The model was successfully applied to Irish, Maltese, and Khaling [klr], as well as some major languages. One interesting observation was that regularization, which normally keeps the model from overfitting, was found to harm performance because the less frequent inflectional patterns look like noise. Regularization tends to guide the model away from those patterns, when it actually needs to learn them. \newcite{silfverberg_encoder-decoder_2018} also investigated the PCFP. With the same amount of input their results were very similar Malouf et al.'s but their model is more realistic for LDD. They do not use the lexeme, which, in morphologically complex languages, is rarely found in naturally-occurring texts. Instead, they trained on inflected forms, which could be extracted from a LDD corpus. 

\begin{figure}[ht]
\label{fig:PCFP}
\begin{center}
\includegraphics[width=0.7\columnwidth]{PCFP.PNG}
\caption{\newcite{silfverberg_encoder-decoder_2018}: Illustration of the Paradigm Cell Filling Problem with Spanish verb inflectional tables. If speakers are only exposed to such partially filled paradigms, how do they fill the missing cell with the correct form?}
\end{center}
\end{figure}

\newcite{kann_neural_2016} use a deep learning approach to morphological re-inflection. Re-inflection attempts to predict a correct inflected form from another form or from morphosyntactic tags or \textit{vice versa}. This is essentially the same task as the approaches to the PCFP, but without reference to the theoretical question, and therefore, is not compelled to limit the input: Kann et al. used completed inflection tables. Their approach does not learn inflectional patterns directly but it outlines possible first steps toward morphological paradigm induction in low resource settings. The model draws inspiration from the theoretical linguistic notion of principal parts \cite{finkel_principal_2007} which refers to minimal subset of forms that allow maximum predictability. This subset can be thought of as the smallest number of inflected forms needed to identify the lexeme's inflectional class. Kann et al. added encoders that incorporate multiple inflected forms, with the assumption that these multiple forms provide complementary information about a pattern and allows the model to predict a complete paradigm. Such ``multi-source'' learning performs better than a single data source by working around ``holes” in data. It can be applied to fully or only partially annotated data.

A similar approach can be applied to low resource settings \cite{ahlberg_semi-supervised_2014,ahlberg_paradigm_2015} using a small number of inflectional tables from Wiktionary which are similar to Table \ref{tab:RuParadigm} but include the whole word instead of the just the inflectional morphemes. With this data, groups of similarly behaving words were extracted from annotated data. Several of these groups were used to identify paradigmatic patterns.  As with Kann et al. the immediate goal is to predict the correct inflected forms of unseen words, but the generalized patterns used to make these predictions are basically inflectional paradigms. The patterns are discovered by abstracting the longest common subsequence in a word and clustering those abstract patterns with same or similar patterns. This is illustrated in Figure \ref{fig:LCS}\footnote{Acknowledgements to Mans Hulden and Ling Lui.}. The three parts of the longest common subsequence (LCS) \textit{rng} or \textit{swm} are extracted from the inflectional table on the left (step 1) and represented abstractly, in this case as \textit{x\textsubscript{1}} and \textit{x\textsubscript{2}}. The abstract symbols replace the longest common subsequence in every word form (step 2). In theory, the characters that remain, in this case \textit{i}, \textit{a}, \textit{u}, are the inflectional morphemes. Since the unique part of the each root morphemes is now represented identically, words with the same inflectional patterns will be identical and can be generalized into paradigmatic patterns (steps 3 and 4). It seems quite possible that exceptions or irregularities could be accounted for by collapsing similar patterns. The experiment was quite successful on some Indo-European languages (German, Spanish, Catalan, French, Galician, Italian, Portuguese, Russian), as well as Maltese and Finnish.

\begin{figure}[ht]
\label{fig:LCS}
\begin{center}
\includegraphics[width=0.7\columnwidth]{Ahlberg2015-LCS.PNG}
\caption{Ahlberg et al. (2015): Abstracting inflectional paradigmatic patterns from inflections and annotated data.}
\end{center}
\end{figure}

Attempts at paradigm induction has contributed to computational morphology in low resource  contexts in a singular way: those attempting it have also attempted to estimate the smallest amount of data necessary to perform it with reasonable accuracy. \newcite{ahlberg_semi-supervised_2014} found that paradigms from the 20 most frequent inflectional classes cover 95\% of German word forms. \newcite{detrez_smart_2012}, who use hand-written rules to fill paradigm slots, started with complete inflectional tables and gradually dropped information from them. In the end, they found that one-two infected forms per paradigm are sufficient to achieve 90\% accuracy in English, Swedish, French, and Finnish. \newcite{silfverberg_encoder-decoder_2018}, using a neural model that learns from a small number of randomly chosen forms per paradigm table, found that two inflected forms gave about 85\% accuracy in Finnish, Georgian, Turkish and some Indo-European languages and three forms bumped the results over 90\% for all but Georgian nouns and Latvian verbs.

\section{Approaches to Computational Morphology}
\label{compMorph}

This section explores the exciting development of computational morphology and their application to low resource languages. The four major approaches are either  rule-based (finite state transducers) or machine learning approaches that induce models from texts. Machine learning strategies differ in whether the texts have been annotated completely (supervised), partially (semi-supervised), or not at all (unsupervised). Until the late-1990s and mid-2000s, the literature on computational morphology was dominated by finite-state machines \cite{kaplan_phonological_1981,koskenniemi_two-level_1983,beesley_finite-state_2003}. Gradually, machine learning started to become popular \cite{cohen_joint_2007,ruokolainen_comparative_2016}. Most early machine learning of morphology used unsupervised, probabilistic models \cite{roark_computational_2007}. Supervised approaches came later, due in part to computer memory limitations \cite{hammarstrom_unsupervised_2011}. Semi-supervised learning has also grown in popularity, due perhaps in part to ``the increased availability of machine-readable inflection tables'' \cite[page 18]{goldsmith_computational_2017}. 

Currently, supervised end-to-end neural networks, or deep learning, are dominating the field. Although deep learning has achieved high results in many NLP tasks, they did not become popular until recently because they train more slowly than non-neural methods for some tasks \cite{cotterell_cross-lingual_2017} and require greater computing power. More significantly, the models and methodology for training have greatly improved. They do require expertise to customize and this still presents a practical hurdle to efficient NLP application in LDD. General linguistics training does not include computer programming and, as yet, few, if any, easy-to-learn graphic user interfaces exist. 

Computational morphological approaches have been tested on a growing number of languages since 2000 \cite{hammarstrom_unsupervised_2011}. The latest CoNLL-SIGMORPHON Shared Tasks \cite{cotterell_cross-lingual_2017,cotterell_conllsigmorphon_2018} indicate how this number has grown. Nevertheless, the task's list of 100+ languages is still small in comparison to nearly 6000 remaining languages. Nor has the number grown in a representative way. The distribution is not proportional to language families or basic morphological types. As an example, the 2018 CoNLL-SIGMORPHON task included only one language classified as polysynthetic, and that language, Navajo, is often classified as agglutinative or fusional.

It is worth noting that comparing the performance of different approaches, and the different models within one approach, is not straightforward. Morphological analysis involves many small steps; if any one step is the basis for evaluation, the assessment changes. For example, a predicted segmentation can miss the correct morpheme boundary by one letter or by many. Or, in labeled segmentation, precision and recall can be a ``micro-average'' measuring the accuracy of segmentation or a ``macro-average'' measuring the accuracy of both segmentation and tagging. For the latter measure, the total count for precision, recall, and F1 measure could be based on types or on tokens. A word type count may obscure whether all types are handled identically. A word token count may make the model seem less accurate if one very frequent word is incorrectly parsed. Some evaluations may consider a parse correct when it produces one of all possible ambiguous parses; others may count a correct parse only when it is the right one in the given context \cite{ruokolainen_supervised_2013}. \newcite{virpioja_empirical_2011} surveyed these issues through five years of MorphoChallenges and found no solution to the varying details, but they concluded that an evaluation based on segmentation is the most simple, robust, and intuitive. Beyond the difficulties caused by vary standards of comparison, computational linguistics evaluates models in two ways. The first is indirect, or extrinsic, evaluation which evaluates the effect a model has on a complete NLP system. This is complicated and time-consuming and so rarely done. Direct, or intrinsic, evaluation is can be performed either automatically against gold standard, annotated data or manually by native speakers or domain expert linguists. \newcite[page 53]{virpioja_empirical_2011} dismiss manual evaluation because human expert decisions are ``subjective'' and ``the amount of work involved restricts its usage.'' This disregards the fact that any gold standard data was originally assembled with the same amount of human subjectivity and hard work. Such attitudes reveals the drawback of intrinsic evaluation. It sets a narrow focus on how a model performs competively against other models on the same dataset, rather than how it performs across datasets or languages. 

\subsection{Finite State Transducers}

Finite state transducers (FST) are a bidirectional, rule-based model that has successfully modeled linguistic patterns \cite{koskenniemi_two-level_1983,beesley_finite-state_2003,hulden_finite-state_2009}. They were the first computational model to successfully both parse and generate word forms \cite{goodman_generation_2013} (see Figure \ref{fig:bidir}). Since they easily represent regular relations between underlying and surface forms, moving left-to-right, they held early importance as a way to represent \newcite{chomsky_sound_1968}'s transformations as rewrite rules \cite{karttunen_2005} and model the theory of two-level morphology.
\bigskip

\begin{figure}[ht]
\label{fig:bidir}
\begin{center}
\includegraphics[width=0.5\columnwidth]{bidirectional.PNG}
\caption{A bidirectional computational morphological model generates an inflected word form from a stem and morphosyntactic tags and parses inflected words forms into stem and tags.}
\end{center}
\end{figure}

FSTs are constructed in two steps. The first step specifies the lexicon and morphotactics in a finite-state lexicon compiler ({\it lexc}), illustrated in Figure \ref{fig:lexc}. This is where concatenative morphological rules and morphological irregularities are addressed. The second step implements morphophonogical rewrite rules. These rules apply changes in specified contexts, allowing the FST to move beyond simple concatenation of morpheme strings and generate well-formed inflected surface forms. For example, a rule would specificy that the final letter in the Arapaho stem \textit{noohow} `see' transforms into a \textit{b} if a vowel-initial morpheme is suffixed after it.
\bigskip
\begin{figure}[ht]
\label{fig:lexc}
\begin{center}
\includegraphics[width=0.95\columnwidth]{FSTlexicon.PNG}
\caption{A snippet of an Arapaho \textit{lexc} file for one stem morpheme \textit{noohow} `see'. The @ symbols surround flag diacritics that communicate how the rewrite rules should handle transformations unique to the stem's inflectional class or other exceptional morphophonological behavior.}
\end{center}
\end{figure}

FSTs remain important to computational morphology. As recently as 2017, it was claimed that computational morphological learning methods are essentially still finite state methods \cite{goldsmith_computational_2017}, They have been found to be superior for many applications over a semi-CRF supervised machine learning model \cite{cotterell_labeled_2015}. With enough linguistic expertise and time for development, FSTs are capable of correctly analyzing any well-formed word in a language. However, FST “grammars” take significant effort to develop, maintain, and update \cite{durrett_supervised_2013,moeller_neural_2018}. Since the ``grammar" must be manually-constructed, FSTs require a thorough knowledge of the language and finite state machines, although in at least one case a FST morphological analyzer has been constructed from labeled inflection tables \cite{forsberg_learning_2016}. 

FSTs work well with languages that have strong constraints on how each lexical category (POS) can be inflected, such as Arapaho, with its unique polysynthetic verb forms. But while the bulk of the FST can be developed relatively quickly with basic descriptive resources (e.g. the Boasian Triad), its reliance on language-specific lexicons and rules means that a full FST grammar needs a language that has been thoroughly documented and described. FSTs can be made to generalize to unseen forms, and if both descriptive resources and a language expert trained to construct the \textit{lexc} and rewrite rules are available, an FST can be a good solution for simple morphological modeling. However, FSTs do not help resolve ambiguity because they output all possible outputs instead of ranking them by probability given surrounding context. An FST's output is always correct but sometimes it more practical for downstream tasks to have one ``best guess''. FSTs cannot be used for active learning easily since they do not learn patterns from new annotated examples. These issues can be resolved with machine learning.

\subsection{Unsupervised Machine Learning}

Unsupervised (computational) learning of morphology (ULM) began as an attempt to prove American structuralism and Bloomfield's ''inductive generalizations,'' as well as a possible language acquisition model \cite{hammarstrom_unsupervised_2011}. ULM models attempt to induce morphological patterns from raw, unannotated texts. They take as input natural language data and, with as little supervision (i.e. parameters, thresholds, human intervention, model selection, etc.) as possible, outputs a ``description'' of the language’s morphological structure, according to \newcite{hammarstrom_unsupervised_2011}. However, their use of the word “description” is a bit misleading. What ULM produces is only a first step to morphological description in general linguistics. \newcite[pp. 33-34]{settles_active_2010} depicts ULM more accurately as exploiting the ``latent structure [in the data] alone to find meaningful patterns'' or features. It outputs the data organized ``in a meaningful way'', usually via a clustering algorithm that finds natural groupings within the data. 

According to \newcite{monson_evaluating_2008}, unsupervised morphological models have followed three main stages of development. Early unsupervised algorithms drew inspiration from Z. Harris \cite{harris_phoneme_1955,harris_morpheme_1967}, who extended a descriptive methodology pioneered by Bloomfield that focuses on what elements in a language can co-occur or not. Harris observed that phonemes do not co-occur unpredictably. The probability of the second phoneme in a word is dictate by the first, the third is dictated by the first two, and so on, illustrated in Figure \ref{fig:harris}. \newcite{harris_phoneme_1955} was the first to use character predictability to identify morpheme boundaries. Second, models inspired by Harris used the Minimum Description Length principle (MDL), a top-down approach that relies on the assumption that words will always be longer than morphemes.  At the third stage of ULM development, models began to leverage patterns akin to inflectional paradigms in order to find inflectional relationship between words and identify their affixes.

\begin{figure}[ht]
\label{fig:harris}
\begin{center}
\includegraphics[width=0.5\columnwidth]{Harris-Entropy-image.PNG}
\caption{Harris' character predictability/entropy. In the word form ``walked" the possible letters that can follow the ``k are more than those that could follow the ``l'', so we predict a morpheme boundary.}
\end{center}
\end{figure}

MDL remains a leading method for unsupervised learning of morphology \cite{hammarstrom_unsupervised_2011}. It extends the Kolgomorov complexity that asks, in essence, ``What's the shortest (in terms of memory) computer program that generates some text?'' MDL simplifies this somewhat abstract question into a weaker but computable model. MDL-based approaches attempt to model the morphological structures by minimizing the memory cost of the input data and the model together. A proposed model encodes hypothesized morpheme strings with binary codes (essentially, a encoded lexicon or ``codebook'' of morpheme candidates) and replaces those strings with the codes whenever they appear in the input texts. The memory cost of the ``codebook'' and the encoded texts are calculated. The best morphological model is the one that where the cost of the ``codebook'' plus the encoded texts have the smallest possible memory cost (see Figure \ref{fig:MDL})\footnote{Generated at https://asr.aalto.fi/morfessordemo/}. Measuring the cost of the model inhibits overlearning and measuring the cost of the data, where the most frequent morphemes have the shortest codes, encourages frequent strings to be identified as repeated morphemes. Two downsides to MDL are that it provides no hint where the model should start looking for morpheme boundaries (what hypothesis it should start with) and it cannot it provide a coherent analysis across a whole language \cite{goldsmith_computational_2017}.

\begin{figure}[ht]
\label{fig:MDL}
\begin{center}
\includegraphics[width=0.9\columnwidth]{MDL2.PNG}
\caption{Minimum Description Length. The model makes random splits (marked by \textit{+} on left) in a corpus, resulting in strings of candidate morphemes, or ``morphs''. A ``codebook'' of binary codes that represent each morph is constructed so that more frequent ``morphemes'' are represented with shorter codes, reducing the average cost per morpheme. Then each occurrence of a morph in the corpus is replaced with its code. Finally, the total cost \textit{C} of the model (codebook) and the data (encoded corpus) is summed. In the one word ``corpus'' on the left, this sum \textit{C} is shown next to the word. (Note: the lowest ``score'' here demonstrates typical segmentation errors in unsupervised learning of morphology.) The random splits that result in the lowest total cost is considered the correct morphological model. The cost of the encoded corpus is calculated by summing the negative log likelihood of each morph's maximum likelihood \textit{$p(m_i)$} (i.e. the count of a morph's \textit{$m_i$} occurrences divided by the total number of morphs in the corpus). The codebook cost is calculated by multiplying the number of the bits \textit{k} needed to code a character by the character length of each morph \textit{$l(m_j)$}. }
\end{center}
\end{figure}

Leading MDL-based models are Linguistica \cite{goldsmith_linguistica:_2000,goldsmith_unsupervised_2001} and the Morfessor family of algorithms \cite{creutz_unsupervised_2005a,creutz_unsupervised_2007}. Linguistica is a foundational work in ULM that attempts to discover “signatures”, shown in Figure \ref{fig:signatures}, which are sets of affixes somewhat akin to inflectional paradigms. Even though it is unsupervised, Linguistica needs to set an affix length limit and this requires at least enough knowledge of the language’s morphology to make a hypothesis. The latest version, Linguistica 5 \cite{lee_linguistica_2016}, attempts to make ULM more accessible by including a graphical user interface and an open-source, modular Python library. The library is meant to extend the model beyond morphology, taking syntactic context into account \cite{nicolai_morphological_2017}. Morfessor \cite{creutz_unsupervised_2005a,creutz_inducing_2005b,creutz_unsupervised_2007} attempts to identify the most likely ``morphs'' (candidate morpheme strings) and the most likely morpheme boundaries. Like many unsupervised models, it builds a kind of a morph lexicon and this reduces their cross-linguistic adaptability. The Morfessor model is built on two parameters that require some hypothesis about the language's morphology: ``(i) our prior belief of the most common morph length, and (ii) our prior belief of the proportion of morph types that occur only once in the corpus'' \cite[page 281]{creutz_unsupervised_2003}. Compared to Linguistica which is less eager to split words, earlier versions of Morfessor used Recursive MDL which tended make too many segments, but later versions of Morfessor stand somewhere in between early Morfessor and Linguistica in terms of segmentation accuracy \cite{creutz_unsupervised_2003}. 

\begin{figure}[ht]
\label{fig:signatures}
\begin{center}
\includegraphics[width=0.5\columnwidth]{Linguistica5-signatures.PNG}
\caption{Screenshot of Linguistica 5. The column on the left displays hypothesized ``affixes'' that were found in complementary distribution on ``stems'', one group of which is displayed on the right.}
\end{center}
\end{figure}

ParaMor is another unsupervised algorithm \cite{monson_unsupervised_2004,monson_paramorMinimally_2007a,monson_paramorfinding_2007b,monson_evaluating_2008}. Like Linguistica, it exploits patterns in the data similar to inflectional paradigms, but unlike Linguistica, it allows multiple segmentations per word. Paramor treats paradigm induction and segmentation separately but it does not address morphophonology as well as Linguistica. It requires more data but not as much memory. \newcite[page 49]{monson_evaluating_2008} claim that Paramor’s unsupervised induction of morphology could facilitate quick development of morphological learners for LRL, but they refrain from explaining how the system, which requires a significant amount of data, could do this.

Most ULM approaches are biased towards a certain affixing pattern or morphological type. Linguistica and ParaMor are tuned to suffixing morphology. Morfessor is better at prefixing+suffixing languages than the other two, but Linguistica does better with suffixing-only languages. All three are most suitable to agglutinative languages but also assume a small number of morphemes per word, often only one prefix/suffix. This does not bode well for morphologically complex languages. 

In theory, unsupervised models are the most exciting and attractive for LDD because they do not require costly data annotation. In fact, an early vision for ULM was to support language technology for minority language communities. This idealistic aim has not materialized because successful ULM demands ``a sufficiently large set of unannotated words in electronic form'' \cite[page 92]{ruokolainen_comparative_2016}. Languages that lack a basic morphological analysis generally lack the amount of raw data that accurate ULM requires. ``Sufficient'' data for unsupervised learning is in the order of a hundreds of thousands or a million words \cite{rocio_detection_2007}. A small corpus is defined as 1,000-100,000 words \cite{kirschenbaum_unsupervised_2012}. Since LRL includes well-documented languages, sometimes obtaining sufficient data may be simply a matter of digitizing existing literature, but for most, a limited written history means that texts must be first recorded orally and then transcribed before this a small corpus is usable. Although language documentation aims to generate a great deal of minimally annotated data \cite{himmelmann_documentary_1998,lehmann_documentation_1999}, LDD projects rarely produce even a 100,000 words and the transcribed portion tends to be much smaller.  Despite this, the claim is still being made that ULM methods, ``provide an inexpensive means of acquiring a type of morphological analysis for low resource  languages'' \cite[page 92]{ruokolainen_comparative_2016}. Another drawback is that unsupervised methods are less accurate than other machine learning approaches. \newcite{soricut_unsupervised_2015} were able to overcome this drawback, at least for some Indo-European languages by using unsupervised word embeddings (word meanings represented in vector space). However, even if word embedding increase accuracy, it is an open question whether they address the large data requirements since successful word embeddings are also built with large amounts of texts. 

\newcite{goldsmith_computational_2017} raise other problematic issues. For example, unsupervised approaches have to start with some assumptions about a language's morphology. Should we start with random parameters or should we use knowledge about the target language? The latter is the most efficient, but using prior linguistic knowledge, as Goldsmith, Lee, and Xanthos and \newcite{palmer_computational_2010} insinuate, means the approach is not truly unsupervised. It also makes ULM even less attractive for under-described languages. With such languages, we can presumably hypothesize from related or geographically adjacent languages, but this opens another question that is not clearly addressed in the literature: how well do current ULM methods transfer across languages, and can we even measure this reliably? This cannot be addressed until ULM models have been tested against a wider range of languages. Currently, English is the \textit{de facto} language in ULM \cite{palmer_computational_2010}. The tension between expanding computational models to a wider range of languages and the prohibitive cost of data to test and evaluate the models will continue to influence the development of computational linguistics unless  better data production methods are devised.

Although unsupervised machine learning is most suitable to resource-rich languages, ULM has been applied in low resource contexts. \newcite{palmer_computational_2010} incorporated unsupervised morphological segmentation as a first step towards semi-automated IGT production (interlinearization) in Uspanteko [usp], a Mayan language of Guatemala with some 4,000 speakers \cite{simons_ethnologue:_2018}. Words were assumed to be morphologically related if they were orthographically similar. Affix candidates were generated by assuming stems are longer than affixes and then filtering for statistically significant co-occurrences. \newcite{moon_unsupervised_2009} also applied ULM to Uspanteko with a method that assumed suffixation only and, most notably, incorporated awareness of document boundaries when generating and clustering candidate morphemes. This technique assumes spelling is likely to be consistent within a document but vary across documents. Considering that minority languages may not have a standardized orthography and transcribers may have little formal education in the language, this is a simple yet very practical twist on the “one sense per discourse” effect used in computational semantics which observes that if a polysemous word such as ``saw'' appears more than once in a document all occurrences are highly likely to express the same sense \cite{gale_one_1992}. Document boundary awareness assumes one inflectional paradigm per stem per document; that is, a morpheme that may belong to two lexical categories is highly likely to appear as only one within a single document. This technique reduces noise and improves results over Morfessor and Linguistica (for English). Remarkably, the performance degrades when the corpus size is increased, perhaps because of increased noise from spurious candidate morphs. \newcite{kirschenbaum_unsupervised_2012} applied ULM to a corpus of Kilivila [kij], an Austronesian language of Papua New Guinea with 20,000 speakers \cite{simons_ethnologue:_2018}. Inspired by \newcite{schone_knowledge-free_2000} and \newcite{Baroni02unsuperviseddiscovery}, they identified morphologically related words by orthographic similarity and a context co-occurence vector. They claim that the method prefers languages with infixes but it is unclear how they determined this with only one language. 

\subsection{Supervised Machine Learning}

Supervised learning of morphology means that models are trained on gold standard annotated data rather than unannotated raw texts. It requires significantly less data than unsupervised learning. However, the data must be annotated. Annotated data makes supervised machine learning qualitatively different than unsupervised learning. Unsupervised learning clusters underlying patterns or features which may facilitate human analysis and description of the data. Supervised learning is trained on data that has already been analyzed and labeled. It learns the labeling and generalizes it, predicting, of with high accuracy, what labels unseen instances should have. 

The best performing non-neural supervised model for sequence prediction tasks such as morphological analysis is conditional random fields (CRF) \cite{lafferty_conditional_2001,muller_efficient_2013,ruokolainen_comparative_2016}. A CRF is a sequence classifier that considers the whole sequence when making a prediction. In morphology, CRFs can work as boundary detection (segmentation) and labeler. According to \newcite{ruokolainen_comparative_2016}, discriminative learning methods such as CRF are better than generative models because they optimize the accuracy of segmentation boundaries and generalize better to unseen forms, assuming they have sufficient training data. A discriminative model's strength lies in its ability to define features beyond the previous label and allow arbitrary weights. CRFs apply logistic regression which allows it to make generalized predictions from arbitrary and possibly dependent features \cite{ruokolainen_supervised_2013}. 

A CRF can look at a number of features as it makes a prediction. A feature function input for a (linear-chain) CRF in morphological labeling might be 1) a whole word segmented into morphemes, 2) the position of the current morpheme in the word, and 2) the label of the previous morpheme. Each feature is weighted during training and, with these weights, labeling predictions are scored over the whole sequence, then transformed into a probability. For example, in a segmentation task where the previous word is ``am/is/are/was/were'', if the target word ends in ``ing'' and a weighted feature is the previous word the CRF might give a high weight to ''-ing'' as a separate morpheme. In labeling, the CRF might decide a word-final ``s'' marks a plural noun and not the 3rd person singular simple present tense by highly weighting the POS tag of the stem. However, the quality of the results relies heavily on the choice of features. This could be a drawback for under-described languages, because if little linguistic description is available then how does one know which linguistic features are optimal? Fortunately, CRFs have been shown to perform reasonably well using primarily language-independent features (i.e. surrounding substrings) \cite{ruokolainen_comparative_2016,moeller_automatic_2018}, including a model trained on approximately 3,000 words of Lezgi [lez], an agglutinative Nakh-Daghestanian language with about 600,000 speakers \cite{simons_ethnologue:_2018}. However, it is not  clear if the performances were more dependent on language-specific or task-specific features.

\begin{figure}[ht]
\label{fig:Features-ML}
\begin{center}
\includegraphics[width=0.95\columnwidth]{Features-ML.PNG}
\caption{Classical machine learning. In feature-based models a human expert must analyze and define optimal features from the data. A CRF usually uses gradient descent to assign weights to those features during training. A discriminative model such as Conditional Random Fields, can assign arbitrary weights.}
\end{center}
\end{figure}

Supervised (and semi-supervised, see section \ref{semi-supervised}) learning requires less data and almost always achieves better results than unsupervised learning \cite{ruokolainen_supervised_2013} and, therefore, seems more promising for LDD. However, it may be necessary to augment textual data with other descriptive resources such as grammars and dictionaries since the size of annotated LDD corpora are still ``inadequate for supervised learning'' \cite[page 18]{duong_natural_2017}. This is not an impractical expectation for LDD since descriptive linguists traditionally concentrate on the Boasian triad: a corpus of interlinearized texts, a grammar, and a dictionary.

\subsection{Supervised Neural Networks or Deep Learning}

  Neural networks (NN), or deep learning, is a family of machine learning techniques that builds layers of perceptrons that can learn complex patterns. Vector representations of the data are received by an input layer, fed into and transform in ``hidden'' layers then arrive at the output, or activation, layer.  Each unit in each layer is connected to each unit in the adjacent layers. The connections are represented by learnable weights; the higher the weight the more influence a unit has on the result. Since deep learning is supervised\footnote{Deep learning morphological segmentation has been performed on unsupervised texts with some success \cite{wang_morphological_2016}} the weights are adjusted via stochasitc gradient descent during backpropagation using ``feedback'' from annotated data. Many flavors of deep learning exist and different models work best for different tasks. Recurrent neural networks (RNN) \cite{Elman91} with long short-term memory gates (LSTM) \cite{Hochreiter_1997} are currently considered state-of-the-art for most NLP tasks. RNNs typically have one layer but recurrently add its output to next input and feed that into the layer, although a deep RNN may have multiple layers and the recurrence occurs only for each time step in the network. This works as a feedback loop. RNNs can input and output sequences of values. A probabilistic prediction of the next item in a sequence is conditioned on the entire history of transformations and previous predictions. Unfortunately, the typical deep learning problem of vanishing gradients, or decay of information through time, is much worse for RNN. That is why the LSTM memory gate is so important. The LSTM decides how much of the previous output should feed into the next transformation and how much should be ``forgotten''. This makes training RNNs much faster because ``forgotten'' information does not needs to be calculated during backpropagation.

\begin{figure}[ht]
\label{fig:DL}
\begin{center}
\includegraphics[width=0.95\columnwidth]{DL.PNG}
\caption{In deep learning, hidden layers create intermediate representation of the data. These essentially serve the function of feature engineering in non-neural machine learning.}
\end{center}
\end{figure}

In morphology, deep learning can be used as classifiers, generators, or sequence-to-sequence mapping. Classifiers classify data points into categories or classes. A data point might be a morpheme and the classes morphosyntactic tags. 
The hidden layers feed into a final logistic function layer (i.e. softmax) that outputs a prediction of the correct class as a probability between 0 and 1. Morphological generation produces fully inflected forms from morphosyntactic or contextual information \cite{cotterell_conll-sigmorphon_2017} or completes morphological paradigms \cite{malouf_generating_2016}. Currently, the strongest generator models use an encoder-decoder, or sequence-to-sequence (seq2seq), architecture of a RNN \cite{sutskever2014,kann_neural_2016}. Encoder-decoders map one sequence to another of a different length by encoding an input sequence of symbols (e.g. the letters of a word) and decoding it as another sequence of symbols (e.g. morphosyntactic tags). Encoder-decoders are also state-of-the-art for morphological tagging \cite{heigold_extensive_2017}, morphological segmentation (including low resource settings) \cite{kann_fortification_2018}, and morphological paradigm completion \cite{cotterell_conllsigmorphon_2018}. 

Until recently, neural networks were considered to have the same drawback as unsupervised learning: large amounts of training data for good results \cite{cotterell_conllsigmorphon_2018}. While it is true, for example, that top performing neural networks generate inflected forms with less than 60\% average accuracy when given only 100 training examples, results can increase significantly (over 85\%) when trained on 1,000 and 10,000 examples (up to 96\%) \cite{cotterell_sigmorphon_2016,cotterell_conll-sigmorphon_2017,cotterell_conllsigmorphon_2018}. Improved results cannot always be accomplished by the tuning tricks common with normal data sizes. For example, adding hidden layers can actually reduce accuracy with smaller amounts of data \cite{cotterell_conll-sigmorphon_2017}. Low data settings require unique techniques that include adjusting the model, training an intermediate step, and artificially augmenting the data. As an example of adjusting the model, \newcite{sudhakar_experiments_2017} found that using a gated recurrent unit (GRU) to control access to previous states in the RNN, rather than the NLP state-of-the-art LSTM, gave better results with up to 1000 training examples. Successful intermediate steps in morphology include training the model to make edits to a string instead of decoding a new sequence, so, for example, the model learns an INSERT operation will generate ``runs'' from ``run''  (these models use some sort of alignment to reduce the number of edit operations between related forms) \cite{makarov_align_2017,makarov_uzh_2018}. Other successful intermediate steps include the whole sentence where an inflected form is or should used or train a model to first produce abstract underlying forms (e.g. ``impossible'' $\rightarrow$ ``in-possible'' $\rightarrow$ ``\textsc{neg}-possible'') the final inflected or parsed form \cite{liu_morphological_2018,moeller_improving_2019}. In recent CoNLL-SIGMORPHON submissions, participants found that these, and other new techniques considerably outperformed the state-of-the-art encoder-decoder baseline \cite{bergmanis_training_2017}. These successes suggests that unique techniques for morphological analysis of low resource languages is a promising area for exploration. A few of these techniques are discussed further in section \ref{augment}. 

\subsection{Semi-Supervised Machine Learning}
\label{semi-supervised}

Unsupervised learning may require less costly annotated data and supervised training may beat unsupervised training in accuracy but combining the two may be the best of both worlds. Semi-supervised, or minimally supervised, learning combines annotated data with a larger set of unannotated data when available annotated data is not adequate to effectively train a supervised model \cite{kohonen_semi-supervised_2010,poon_unsupervised_2009}. Like unsupervised learning, semi-supervised learning exploits the underlying structure of raw texts, but with the goal of improving the machine’s predictions, rather than discovering new ones \cite{settles_active_2010}. 

Semi-supervised learning has been widely researched for computational morphology and a wide variety of techniques have been employed \cite{ruokolainen_comparative_2016}. Some ULM algorithms, such as Morfessor Baseline, have proven adaptable to semi-supervised learning \cite{kohonen_semi-supervised_2010}. \newcite{dreyer_discovering_2011} used a ``mostly unsupervised'' method that employs a generative probabilistic model and 10 million unannotated words. It does POS-tagging, segments and tags morphemes, and identifies candidate inflectional paradigms. \newcite{ahlberg_semi-supervised_2014} predicted general paradigmatic patterns using longest common substrings and some unannotated data (LCS) (see section \ref{paradigms} for more details). The latter two models presuppose a few completed inflectional paradigms which are provided by native speakers either directly, or as computational linguists tend to prefer, through the indirect means of an expert who pre-processes the data or through crowdsourced resources such as Wiktionary.

While semi-supervised learning was early praised for its greater effectiveness and practicability, real applications were rare well into the late 2000’s \cite{druck_reducing_2007}. One reason may be that, like unsupervised learning which requires some initial hypothesis, many semi-supervised models make strong assumptions about the data. These assumptions may hold true in pre-processed datasets but ``tend to be violated in real-world data'' \cite[page 1]{druck_reducing_2007}. In addition, unannotated and annotated data cannot be simply combined together. Annotated data has to be weighted so that the larger, unannotated part does not overwhelm it. Overall, results in semi-supervised learning ``strongly suggest that it is crucial to use the few available annotated training instances as efficiently as possible before…incorporating large amounts of unannotated data'' \cite[page 35]{ruokolainen_supervised_2013}. 

Semi-supervised learning is promising for LDD because even though small amounts of annotated data are ``easy to get by manual annotation'' \cite[page 49]{virpioja_empirical_2011}, typical documentation corpora are only partially annotated. However, the results suggest that all available descriptive data for LRL should be exploited as much as possible before relying on unsupervised data. This heavy dependence on annotated data still poses a challenge \cite{andrews_bayesian_2017}. 

\section{Exploiting Resources}
\label{resources}

This section explores ways to improve results for some of the approaches discussed in section 2 when applied in low resource  contexts. The high cost of annotating texts or manually constructing FSTs may be prohibitive,  and even though supervised and semi-supervised methods requires less data than unsupervised or neural network models, they still need significantly more data than is typically available for low resource languages. ``Understanding the available resources is the first step in building a natural language processing framework for a target low resource language'' \cite[page 13]{duong_natural_2017}. The spotty nature of LDD data necessitates using whatever resources are available \cite{palmer_semi-automated_2009}. This section present some techniques to exploit available resources other than textual data, such as artificially generated ``textual'' data, descriptive linguistic knowledge, and resources in other languages. 

\subsection{Non-Textual Resources}

Even though annotated textual data is scarce for most languages, many have been described to some extent and some linguistic resources exist for them. In practice, language documentation projects are rarely undertaken (and more rarely funded) except to support descriptive analysis and publication \cite{thieberger_using_2012,austin_language_2014,vallejos_integrating_2014,thieberger_assessing_2016}. Describing a language’s morphological structure is relatively easy \cite{roark_computational_2007} and linguists tend to tackle it in the first stages. Minimally, a few inflectional paradigms can be easily elicited in a language documentation project. The bulk of a language’s morphology is described in structured data. Structured data might be any organized or pre-processed data that is not a naturally occurring text, such as inflectional tables or word lists.

Crowdsourced websites such as Wiktionary sometimes have inflectional tables for LRL. \newcite{cotterell_labeled_2015} successfully exploit spell checkers and Wiktionary. \newcite{ahlberg_semi-supervised_2014} use publically available inflection tables when combining semi-supervised learning with a feature-based Support Vector Machine (SVM). \newcite{durrett_supervised_2013} also exploit Wiktionary for inflectional tables as the supervised part of a semi-supervised model. They claim that this allows their model to ``extend to other languages [with inflectional tables in Wiktionary] without change" but it is to be expected some of the 150 or so languages on Wiktionary have very few tables. 

If data is thoroughly annotated, computational morphology can improve results from labeling that are not strictly morphological in nature. For example, \newcite{muller_efficient_2013} reduced CRF training time by performing a coarse POS-tagging then fine-grained morpheme tagging. The POS-tagging leaves the algorithm with fewer possible tags to process for each instance. 

Exploiting non-textual resources does not always improve results. Semi-supervised learning with a discriminative sequence learner like a CRF performs worse when non-textual resources are integrated because the external resources receive undue weight. \newcite{andrews_bayesian_2017} claim that models, such as a Maximum Entropy Hidden Markov Model, that generate a lexicon are less likely to degrade in performance because the lexicon essentially acts as an external resource and does not compete with the annotated corpus. 

\subsection{Transfer and Joint Learning}
\label{transfer}

Transfer learning is a common technique for low resource languages that can be applied in semi-supervised and unsupervised learning \cite{duong_natural_2017}. Transfer learning assumes that what succeeds on one language will work well on another and attempts to transfer that success. It may apply one model to another language or attempt to transfer annotated tags by aligning words across languages \cite{duong_natural_2017}. Transfer learning performs better when trained on more than one source language. \newcite{kann_fortification_2018} found that single segmentation model trained simultaneously on four related polysynthetic languages performed as well as and sometimes better than single language models.

\newcite{baumann_using_2014} applied transfer learning principles by using an English word list to identify affixes in unlabeled data of Tagalog [tgl] and Zulu [zul], low resource languages (but clearly not endangered -- both have over 20 million speakers) spoken in the Philippines and South Africa respectively. The strategy was motivated by the realization that many low resource languages are spoken in multi-lingual contexts and have borrowed vocabulary from more economically powerful languages. A word list in that language identifies some root morphemes. Splitting substrings from these roots provides a list of prefixes and suffixes. With some knowledge of the language's morphological patterns, it was even possible to identify infixes in Tagalog. However, the strategy does not do well when a word has multiple affixes. 

Transfer learning is promising for low resource languages but it has its limitations. When RNNs performed cross-lingual morphological tagging on 18 languages from four language families, the conclusion was informative but perhaps unsurprising to those familiar with linguistic typology—the closer the languages' relationship, the more accurate were the results \cite{cotterell_cross-lingual_2017}. The precise correlation between linguistic genetics and results in transfer learning is largely unexplored \cite{buys_cross-lingual_2016,cotterell_cross-lingual_2017}, although interest is growing and new experiments are forthcoming \cite{sigmorphon2019}.  It is still not clear, for example, whether morphological structure or phonological (orthographic) similarity is a stronger factor. It \textit{is} clear that the amount of morphologically marked information common across languages influences results, so if a word ``in the source language does not overtly mark a grammatical category [that is marked] in the target language, it is nigh impossible to expect a successful transfer'' \cite[page749]{cotterell_cross-lingual_2017}. Transfer learning shows sharply the difficulties in paradigm induction.  Paradigms do not generalize across languages. Each language has its own unique ``layout'' for classes within each lexical category and these can differ considerably even in closely-related languages.

Since transfer learning can potentially overcome deep learning’s data hunger, discovering how to improve deep learning results in when transferring from high resource  to low resource settings would be ``a boon for low resource computational linguistics'' \cite[page 752]{cotterell_cross-lingual_2017}. \newcite{duong_natural_2017} points out that automatic projection of annotation (specifically, POS-tagging, noun phrase chunking, and dependency parsing) between high and low resource languages admits error at multiple points. The results have to be corrected or adjusted. Any projection-based method requires some seed data to perform well \cite{buys_cross-lingual_2016}, so one question ripe for exploration is how much data does the low resource target language need to balance a high resource  source language's greater data? \newcite{cotterell_cross-lingual_2017} claim only a “small amount of annotation” is required; but they also claim the necessary annotation should not take too long. Conspicuously, they refrain from defining “small amount” and “not too long”. 

Joint learning trains simultaneously on different types of linguistic information. For example, since segmentation and tagging are generally separate task in computational morphology,  training a semi-CRF to do both tasks would be considered joint learning \cite{cotterell_labeled_2015}. It is based on the intuition that one type of linguistic knowledge (e.g. syntax) can improve results in another domain (e.g. morphology), and vice versa \cite{goldsmith_computational_2017}. One of the earliest joint learning attempts was \newcite{schone_knowledge-free_2000}'s incorporation of semantic, orthographic, and syntactic information into unsupervised learning of morphology. The semantic information came from Latent Semantic Analysis which represents meanings as patterns of words that appear together in text, for example ``morphology'', ``morpheme'', ``inflection'''. Semantic information helped avoid the typical unsupervised segmentation error (e.g \textit{all-y} instead of \textit{ally}). It was supplemented by the probability of two affixes alternating on one ``stem'', which was calculated using orthographic information. Syntactic information was derived from the frequency of words that occur around pairs of possibly related inflected forms. The output was ``conflation sets'' which are paradigm-like groups (e.g. ``abuse'', ``abused'', ``abuses'', ``abusing'').
 
Since morphology carries a great deal, and in some languages most, syntactic information, syntax is important information for morphological analysis. Joint learning of syntax and morphology can be as simple as incorporating POS tags as features or extending POS tagging methods to morphological tagging \cite{buys_cross-lingual_2016,cotterell_cross-lingual_2017}. Syntax can also be incorporated into supervised learning by clustering words with similar final substrings which serve as a proxy for grammatical agreement and are assumed to indicate lexical categories \cite{lee_modeling_2011}. Successful joint learning of semantics and morphology has used the semantic information in word embeddings \cite{soricut_unsupervised_2015}. 

Joint learning in low resource settings has been successful applied using information commonly produce by LDD projects \cite{palmer_semi-automated_2009,moeller_automatic_2018}, particularly POS tags.

\subsection{Data Augmentation}
\label{augment}

Another ``resource'' to exploit is artificially generated data to augment naturally occurring textual data. \newcite[page 38]{bergmanis_training_2017} found that the ``main benefit of the various data augmentation methods is providing a strong bias towards … regularizing the model, with a slight additional benefit obtained by learning the typical character sequences in the language." \newcite{kann_fortification_2018} found that with data augmentation a neural model can outperform Morfessor and match CRF accuracy. The success of these methods suggests that data augmentation may be a fruitful area of experimentation for applying neural models in low resource settings.

The most successful data augmentation strategies in low resource settings bias an encoder-decoder model to copy strings \cite{bergmanis_training_2017,kann_fortification_2018,makarov_align_2017,makarov_uzh_2018}. For example, \newcite{bergmanis_training_2017} and \newcite{kann_fortification_2018} auto-encode (train a model to output a string identical to the input) words from the training corpus. Their performance varied between languages, perhaps due to how well a language’s morphological features were represented in the small training datasets, but \newcite{bergmanis_training_2017} found this method performed better than transfer learning and \newcite{kann_fortification_2018}, who refer to the approach as ``multi-task learning'', found it superior to \newcite{silfverberg_data_2017}'s strategy of replacing common substrings in the test data with random strings from the training data.  Simplistically, we might assume that copying ``base forms" (e.g. ``run'') would work well for morphologically simple languages that add few affixes to the base form, but the foundation of morphological analysis is the systematic patterns of inflection which means that all languages tend to repeat identical or very similar substrings across morphologically related forms. 

\section{Computational Morphology and Language Documentation and Description}
\label{CLLDD}

Computational linguistics has a solid history of success in morphological analysis and paradigm learning. It is showing significant improvement in low resource settings. The success of computational morphology in low resource settings could benefit efforts to document and describe new languages. However, documentary and descriptive linguistics has not witnessed much evidence of this success. This is doubly unfortunate because computational linguistics stand to benefit from the increased annotated data in new languages that would result. This section describes the current situation and then outlines some tools and techniques for addressing the issue of limited language data which plagues both fields.

The morphological annotation needed to develop both general and computational linguistics is performed primarily by hand. While manual annotation provides the most reliable analysis---a human is much better than a computer at interpreting contextual cues---it is the least effective method \cite{Baldridge06,baldridge_how_2009,palmer_semi-automated_2009}. Manual annotation is repetitive, monotonous, and, therefore, prone to typos and inconsistencies. It is costly, requiring money to create guidelines and hire and train annotators. And it is time-consuming. Annotating just one layer of the Penn Treebank \cite{taylor_penn_2003}, a popular English data source for computational linguistics, took three years \cite{duong_natural_2017,he_human---loop_2016}. LDD projects rarely annotate all the data they record within their alloted time and grants. 

Currently, the most widely used programs for linguistic annotation and initial analysis are ELAN \cite{auer_elan_2010}, pictured in Figure \ref{fig:ELAN}\footnote{Acknowledgements: \newcite{bouda_corpus_2012}}, and FLEx \cite{Black06FLEX}, pictured in Figure \ref{fig:FLEX}\footnote{Source: http://software.sil.org/fieldworks/resources/tutorial/interlinearize-texts/}. Neither software uses any machine learning approach. They suggest segmentations and morpheme glosses (tags) only when the user is analyzing a word form that is identical to one that was previously analyzed or if the user first manually constructs morphological rules. 

\begin{figure}[ht]
\label{fig:ELAN}
\begin{center}
\includegraphics[width=0.75\columnwidth]{ELAN.PNG}
\caption{Morphological analysis and annotation in ELAN.}
\end{center}
\end{figure}

\begin{figure}[ht]
\label{fig:FLEX}
\begin{center}
\includegraphics[width=0.75\columnwidth]{FLEX.PNG}
\caption{Morphological analysis and annotation in Fieldworks Language Explorer (FLEx).}
\end{center}
\end{figure}

\newcite{baldridge_how_2009} propose two stratagems to improve and speed annotation: reducing the cost of annotation per instance or getting more miles out of previously annotated data. In computational linguistics crowdsourcing is a typical example of the first strategem. Crowdsourcing has been attempted in language documentation but has proven less effective than in high resource contexts \cite{bird_aikuma:_2014,bettinson_developing_2017}. Fewer speakers are available in minority communities and even fewer have adequate technical literacy or are interested in maintaining their language. For the second strategy, Baldridge et al. recommend active learning. Active learning is a machine learning technique that directs human annotators towards the most informative unannotated instances (i.e. the forms that are most dissimilar to what the machine has been trained on). Active learning also allows annotators to vet and correct the machine predictions. The active input makes machine learning models more accurate with less annotation effort. Experiments in computer-aided machine translation \cite{kothur_document-level_2018} and interlinearizing LDD data \cite{palmer_semi-automated_2009,palmer_evaluating_2009,palmer_computational_2010} indicate that active learning is an promising route for increasing annotated data. For example, after training a machine learning model on available Uspanteko data, \newcite{palmer_semi-automated_2009} had the machine displays its best guess or suggests the most likely morphosyntactic label to annotators. Their responses were used to re-train the model and give it a significant boost in accuracy. Once an annotator is familiar with vetting or correcting a machine’s prediction, the process is faster than unaided annotation and more accurate than completely automatic annotation. \newcite{rocio_detection_2007} even found that if patterns of errors are identified manually, the correction process can also be automated.

If successful computational approaches to morphology were incorporated into documentary and descriptive linguistics, new data could be more quickly and consistently annotated, analyzed and described. There are several ways that this could be achieved but two fundamental components are missing. First, the most effective machine learning approaches for low resource languages are not available to linguists with no computational background. This could be achieved by a separate software program compatible with popular tools, as, for example, the web-based ELPIS \cite{foley_elpis_2018} transcription app for low resource languages which intakes transcribed corpora and outputs its predictions in ELAN-compatible files. Or it could be incorporated as an add-on to another software. 

Second, user interfaces need to developed that allow active learning with the incorporated machine learning models. For example, FLEx already uses a green shade to indicate unconfirmed word analyses. It could expand that to reflect a machine learning model's ``confidence'' in its predictions and direct the users' attention to the least confident, or most informative, suggestions that need to be vetted or corrected. If a segmentation model was fed ``indescribably'' it would hopefully output ``in-describ-ab-ly'' with a high probability which would could be coded as a light green, while the incorrect output ``in-describ-ably'' would be darker green or perhaps red. Another option is to develop an interface similar to what \newcite{palmer_semi-automated_2009} used. Users were presented only with the most informative label predictions, along with a ranked list of other labels. The model was repeatedly re-trained and tested and each time the users were presented with new predictions to vet.

Once documentary and descriptive linguists have access to machine learning models and active learning strategies for morphological analysis and annotation, these same two components can be applied to the next steps in linguistic description. Using the annotated corpora, an logical next step would be to offer machine hypotheses about a language's inflectional classes and their paradigm tables. Again, this might be built as a separate program or built into ELAN or FLEx which already have features that help users organize and analyze their data. Active learning strategies could be incorporated that allow users to examine the machine's hypotheses as the annotation increases and confirm or reject predicted paradigm tables and classes of stems.

Bridging the gap between computational and ``pen-and-paper'' linguists will open questions for future research. For example, how will the new tools affect recommended best practices for linguistic field methods? The answers could guide field linguists to focus their limited budget on the activities that are most likely to facilitate computational assistance. Of course, guidance from the computational side must weigh against practical concerns. For example, syntactic parsing may prove highly beneficial for joint learning, but training a speaker to do syntactic annotation might turn out to be more costly than another, less optimal task. Another question is how accurate does the machine need to be? It would be wonderful to begin machine-aided morphological analysis as soon as transcribed data is available, but below a certain accuracy threshold computer support is more time-consuming than manual annotation \cite{kothur_document-level_2018,palmer_semi-automated_2009}. The optimal threshold will probably vary depending on language family or morphological type and many experiments have yet to be run. The results could lead to new guidelines on how much field recordings and annotation a field project should budget for.

\section{Conclusion}

Until very recently, success in computational morphology depended on high resource contexts, which was only a trivial percentage of the world’s approximately 7000 languages could provide. This retards the development of computational linguistics and its potential contribution to broader linguistic science. Recently, computational approaches are being increasingly applied with reasonable success to low resource languages. Promising machine learning techniques for limited data settings are being discovered. 

Our understanding of morphological structure in the world's languages rests on empirical studies of language data. Computational morphology, in particular, will not achieve helpful results without sufficient published data. The most efficient computational models are served best by publically available morphologically annotated corpora, because, although unsupervised models achieve some success with unannotated data, accuracy is much higher in supervised and semi-supervised machine learning. Fortunately, morphological annotation (interlinearization) of texts serves as a foundation to traditional linguistic analysis. Documentary and descriptive linguistics also regularly produce other structured data resources such as grammars and dictionaries. Semi-supervised learning, in particular, are well suited to contexts where a combination of textual and non-textual linguistic data exists. Exploiting a variety of resources and a range of machine learning models is a creative way to improve accuracy with limited data. Nevertheless, for all models—finite state transducers, unsupervised, supervised feature-based or neural networks, and semi-supervised—more data means better results. 

Despite the importance of annotated data, the computational linguistics literature repeatedly returns to the unfortunate reality that ``annotating sufficient data...is expensive'' \cite[page 1954]{cotterell_cross-lingual_2017} and retards the development of computational models. To this the literature often adds another perceived inconvenience: that annotation ``relies on linguistic expertise'' (\textit{ibid}). A language expert—native speaker or linguist—must be involved in recording, analyzing, and annotating data to produce the gold standards annotations necessary for training and evaluating machine learning models or for building finite state machines. Strange as it may seem, it is fortunate these same costs and inconveniences have long stood as a bottleneck to documentary and descriptive linguistics data production \cite{bird_documentary_2015,bettinson_developing_2017,holton_developing_2017}. The bottleneck hinders linguistic description, empirical experiments, theory development, and community efforts at language revitalization and maintenance, as well as the development of computational models and methods. The fortunate part is that the inconvenient reliance on people who actually know the language has been an integral part of linguistics for as long as it has existed as a science. Language documentation and descriptive linguistic literature is brimming with proven field methods for making the most of human linguistic expertise. What remains, then, is to reduce the cost of annotation. And this relies on the expertise of computational linguists and the success of computational morphology in low resource contexts.

\newpage

\bibliographystyle{acl}
\bibliography{References}
\end{document}
